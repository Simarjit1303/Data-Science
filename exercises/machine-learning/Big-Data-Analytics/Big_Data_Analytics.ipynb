{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDewbIozz2mDV2wTahRb15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simarjit1303/Data-Science/blob/main/Big_Data_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus\n",
        "\n",
        "corpus_names = [name for name in dir(nltk.corpus) if not name.startswith('_') and hasattr(getattr(nltk.corpus, name), 'fileids')]\n",
        "\n",
        "# Print each identified corpus name on a new line\n",
        "for i, name in enumerate(corpus_names):\n",
        "    print(f\"{i+1}. {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi_QJk7YYNo3",
        "outputId": "d9324dce-e4e6-44d7-982e-c1565a04017f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. AlignedCorpusReader\n",
            "2. AlpinoCorpusReader\n",
            "3. BCP47CorpusReader\n",
            "4. BNCCorpusReader\n",
            "5. BracketParseCorpusReader\n",
            "6. CHILDESCorpusReader\n",
            "7. CMUDictCorpusReader\n",
            "8. CategorizedBracketParseCorpusReader\n",
            "9. CategorizedCorpusReader\n",
            "10. CategorizedPlaintextCorpusReader\n",
            "11. CategorizedSentencesCorpusReader\n",
            "12. CategorizedTaggedCorpusReader\n",
            "13. ChasenCorpusReader\n",
            "14. ChunkedCorpusReader\n",
            "15. ComparativeSentencesCorpusReader\n",
            "16. ConllChunkCorpusReader\n",
            "17. ConllCorpusReader\n",
            "18. CorpusReader\n",
            "19. CrubadanCorpusReader\n",
            "20. DependencyCorpusReader\n",
            "21. EuroparlCorpusReader\n",
            "22. FramenetCorpusReader\n",
            "23. IEERCorpusReader\n",
            "24. IPIPANCorpusReader\n",
            "25. IndianCorpusReader\n",
            "26. KNBCorpusReader\n",
            "27. LinThesaurusCorpusReader\n",
            "28. MTECorpusReader\n",
            "29. MWAPPDBCorpusReader\n",
            "30. MacMorphoCorpusReader\n",
            "31. NKJPCorpusReader\n",
            "32. NPSChatCorpusReader\n",
            "33. NombankCorpusReader\n",
            "34. NonbreakingPrefixesCorpusReader\n",
            "35. OpinionLexiconCorpusReader\n",
            "36. PPAttachmentCorpusReader\n",
            "37. PanLexLiteCorpusReader\n",
            "38. PanlexSwadeshCorpusReader\n",
            "39. Pl196xCorpusReader\n",
            "40. PlaintextCorpusReader\n",
            "41. PortugueseCategorizedPlaintextCorpusReader\n",
            "42. PropbankCorpusReader\n",
            "43. ProsConsCorpusReader\n",
            "44. RTECorpusReader\n",
            "45. ReviewsCorpusReader\n",
            "46. SemcorCorpusReader\n",
            "47. SensevalCorpusReader\n",
            "48. SentiWordNetCorpusReader\n",
            "49. SinicaTreebankCorpusReader\n",
            "50. StringCategoryCorpusReader\n",
            "51. SwadeshCorpusReader\n",
            "52. SwitchboardCorpusReader\n",
            "53. SyntaxCorpusReader\n",
            "54. TaggedCorpusReader\n",
            "55. TimitCorpusReader\n",
            "56. TimitTaggedCorpusReader\n",
            "57. ToolboxCorpusReader\n",
            "58. TwitterCorpusReader\n",
            "59. UdhrCorpusReader\n",
            "60. UnicharsCorpusReader\n",
            "61. VerbnetCorpusReader\n",
            "62. WordListCorpusReader\n",
            "63. WordNetCorpusReader\n",
            "64. WordNetICCorpusReader\n",
            "65. XMLCorpusReader\n",
            "66. YCOECorpusReader\n",
            "67. abc\n",
            "68. alpino\n",
            "69. bcp47\n",
            "70. brown\n",
            "71. cess_cat\n",
            "72. cess_esp\n",
            "73. cmudict\n",
            "74. comparative_sentences\n",
            "75. comtrans\n",
            "76. conll2000\n",
            "77. conll2002\n",
            "78. conll2007\n",
            "79. crubadan\n",
            "80. dependency_treebank\n",
            "81. extended_omw\n",
            "82. floresta\n",
            "83. framenet\n",
            "84. framenet15\n",
            "85. gazetteers\n",
            "86. genesis\n",
            "87. gutenberg\n",
            "88. ieer\n",
            "89. inaugural\n",
            "90. indian\n",
            "91. jeita\n",
            "92. knbc\n",
            "93. lin_thesaurus\n",
            "94. mac_morpho\n",
            "95. machado\n",
            "96. masc_tagged\n",
            "97. movie_reviews\n",
            "98. multext_east\n",
            "99. names\n",
            "100. nombank\n",
            "101. nombank_ptb\n",
            "102. nonbreaking_prefixes\n",
            "103. nps_chat\n",
            "104. opinion_lexicon\n",
            "105. perluniprops\n",
            "106. ppattach\n",
            "107. product_reviews_1\n",
            "108. product_reviews_2\n",
            "109. propbank\n",
            "110. propbank_ptb\n",
            "111. pros_cons\n",
            "112. ptb\n",
            "113. qc\n",
            "114. reuters\n",
            "115. rte\n",
            "116. semcor\n",
            "117. senseval\n",
            "118. sentence_polarity\n",
            "119. sentiwordnet\n",
            "120. shakespeare\n",
            "121. sinica_treebank\n",
            "122. state_union\n",
            "123. stopwords\n",
            "124. subjectivity\n",
            "125. swadesh\n",
            "126. swadesh110\n",
            "127. swadesh207\n",
            "128. switchboard\n",
            "129. timit\n",
            "130. timit_tagged\n",
            "131. toolbox\n",
            "132. treebank\n",
            "133. treebank_chunk\n",
            "134. treebank_raw\n",
            "135. twitter_samples\n",
            "136. udhr\n",
            "137. udhr2\n",
            "138. universal_treebanks\n",
            "139. verbnet\n",
            "140. webtext\n",
            "141. wordnet\n",
            "142. wordnet2021\n",
            "143. wordnet2022\n",
            "144. wordnet31\n",
            "145. wordnet_ic\n",
            "146. words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "pBbqo3KLcPn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_unique_synonyms(word):\n",
        "    \"\"\"Get all unique synonyms (lemmas) for a given word using WordNet.\"\"\"\n",
        "    synonyms = set()  #\n",
        "\n",
        "\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            synonyms.add(synonym.lower())\n",
        "\n",
        "    return sorted(synonyms)\n",
        "\n",
        "\n",
        "word = \"car\"\n",
        "synonyms = get_unique_synonyms(word)\n",
        "\n",
        "\n",
        "print(f\"Unique synonyms for '{word}':\")\n",
        "for i, synonym in enumerate(synonyms, 1):\n",
        "    print(f\"{i}. {synonym}\")\n",
        "\n",
        "print(f\"\\nTotal unique synonyms found: {len(synonyms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMwYTRDcQgw",
        "outputId": "167552cd-8709-46da-ee5e-5ba74f682974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique synonyms for 'car':\n",
            "1. auto\n",
            "2. automobile\n",
            "3. cable car\n",
            "4. car\n",
            "5. elevator car\n",
            "6. gondola\n",
            "7. machine\n",
            "8. motorcar\n",
            "9. railcar\n",
            "10. railroad car\n",
            "11. railway car\n",
            "\n",
            "Total unique synonyms found: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def get_unique_synonyms(word):\n",
        "    \"\"\"Get all unique synonyms (lemmas) for a given word using WordNet.\"\"\"\n",
        "    synonyms = set()\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            synonyms.add(synonym.lower())\n",
        "    return sorted(synonyms)\n",
        "\n",
        "def replace_with_synonyms_random(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged = pos_tag(words)\n",
        "\n",
        "    replaceable_words = []\n",
        "    for i, (word, tag) in enumerate(tagged):\n",
        "        if word.isalpha():\n",
        "            synonyms = get_unique_synonyms(word)\n",
        "            filtered_synonyms = [s for s in synonyms if s != word.lower()]\n",
        "            if filtered_synonyms:\n",
        "                replaceable_words.append((i, filtered_synonyms))\n",
        "\n",
        "    if replaceable_words:\n",
        "        n = random.randint(1, len(replaceable_words))\n",
        "        random.shuffle(replaceable_words)\n",
        "\n",
        "        for i, synonyms in replaceable_words[:n]:\n",
        "            words[i] = random.choice(synonyms)\n",
        "\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nLO-gBITu0S",
        "outputId": "7f90039c-c881-4c61-e8d0-67326b942903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Enter Sentence: - \")\n",
        "new_sentence = replace_with_synonyms_random(sentence)\n",
        "print(\"Original:\", sentence)\n",
        "print(\"Modified:\", new_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFu3oZUUX87_",
        "outputId": "773fcbbc-73bd-4398-c9d4-f25ec29436a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Sentence: - The smart student solved a difficult problem quickly.\n",
            "Original: The smart student solved a difficult problem quickly.\n",
            "Modified: The smart educatee solved a hard job rapidly .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "name_entity_recog = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "def extract_entities(word):\n",
        "  doc = name_entity_recog(word)\n",
        "  print(\"Entities found: \")\n",
        "  for ent in doc.ents:\n",
        "    print(f\"{ent.text} --> {ent.label_}\")"
      ],
      "metadata": {
        "id": "zMbN6Sbfa97T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Narendra Modi was born in Gujarat and is serving as prime minister of the India.\"\n",
        "extract_entities(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ7CMXTCc2lO",
        "outputId": "555ebfbd-d19f-4407-b6c9-9d04c31f6507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found: \n",
            "Narendra Modi --> PERSON\n",
            "Gujarat --> GPE\n",
            "India --> GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "name_entity_recog = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample replacements for some entity types\n",
        "entity_replacements = {\n",
        "    \"PERSON\": [\"Elon Musk\", \"Taylor Swift\", \"Barack Obama\", \"Emma Watson\"],\n",
        "    \"GPE\": [\"India\", \"Germany\", \"Brazil\", \"Japan\", \"Canada\"],\n",
        "    \"LOC\": [\"Sahara Desert\", \"Amazon Rainforest\", \"Mount Everest\"]\n",
        "}\n",
        "\n",
        "def extract_and_replace_entities(text):\n",
        "    doc = name_entity_recog(text)\n",
        "    print(\"Entities found:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"{ent.text} --> {ent.label_}\")\n",
        "\n",
        "    new_text = text\n",
        "    replaced_entities = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        ent_type = ent.label_\n",
        "        ent_text = ent.text\n",
        "\n",
        "        if ent_type in entity_replacements:\n",
        "            if ent_text not in replaced_entities:\n",
        "                options = [e for e in entity_replacements[ent_type] if e.lower() != ent_text.lower()]\n",
        "                if options:\n",
        "                    replaced_entities[ent_text] = random.choice(options)\n",
        "            if ent_text in replaced_entities:\n",
        "                new_text = new_text.replace(ent_text, replaced_entities[ent_text])\n",
        "\n",
        "    print(\"\\nModified Text:\")\n",
        "    print(new_text)\n"
      ],
      "metadata": {
        "id": "Hf3fXiEme4xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Narendra Modi was born in Gujarat and is serving as prime minister of the India.\"\n",
        "extract_and_replace_entities(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AlKch9Je_n9",
        "outputId": "ada60e58-61e2-40db-e83a-2b8d80f6af9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found:\n",
            "Narendra Modi --> PERSON\n",
            "Gujarat --> GPE\n",
            "India --> GPE\n",
            "\n",
            "Modified Text:\n",
            "Emma Watson was born in Canada and is serving as prime minister of the Brazil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "def tokenizing_word(word):\n",
        "  tokenizer = WhitespaceTokenizer()\n",
        "  return tokenizer.tokenize(word)\n",
        "\n",
        "\n",
        "tokens = tokenizing_word(input(\"Enter senetence: \"))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkpwcqEGpKYN",
        "outputId": "165cdf59-d7ac-464e-e5e3-fc520407b2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter senetence: Yash is living in Berlin, Germany \n",
            "['Yash', 'is', 'living', 'in', 'Berlin,', 'Germany']\n"
          ]
        }
      ]
    }
  ]
}
