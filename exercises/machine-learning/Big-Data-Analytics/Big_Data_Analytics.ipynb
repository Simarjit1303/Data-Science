{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0WHWYfPlkoZFh6wc4Fync",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simarjit1303/Data-Science/blob/main/exercises/machine-learning/Big-Data-Analytics/Big_Data_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus\n",
        "\n",
        "corpus_names = [name for name in dir(nltk.corpus) if not name.startswith('_') and hasattr(getattr(nltk.corpus, name), 'fileids')]\n",
        "\n",
        "# Print each identified corpus name on a new line\n",
        "for i, name in enumerate(corpus_names):\n",
        "    print(f\"{i+1}. {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi_QJk7YYNo3",
        "outputId": "d9324dce-e4e6-44d7-982e-c1565a04017f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. AlignedCorpusReader\n",
            "2. AlpinoCorpusReader\n",
            "3. BCP47CorpusReader\n",
            "4. BNCCorpusReader\n",
            "5. BracketParseCorpusReader\n",
            "6. CHILDESCorpusReader\n",
            "7. CMUDictCorpusReader\n",
            "8. CategorizedBracketParseCorpusReader\n",
            "9. CategorizedCorpusReader\n",
            "10. CategorizedPlaintextCorpusReader\n",
            "11. CategorizedSentencesCorpusReader\n",
            "12. CategorizedTaggedCorpusReader\n",
            "13. ChasenCorpusReader\n",
            "14. ChunkedCorpusReader\n",
            "15. ComparativeSentencesCorpusReader\n",
            "16. ConllChunkCorpusReader\n",
            "17. ConllCorpusReader\n",
            "18. CorpusReader\n",
            "19. CrubadanCorpusReader\n",
            "20. DependencyCorpusReader\n",
            "21. EuroparlCorpusReader\n",
            "22. FramenetCorpusReader\n",
            "23. IEERCorpusReader\n",
            "24. IPIPANCorpusReader\n",
            "25. IndianCorpusReader\n",
            "26. KNBCorpusReader\n",
            "27. LinThesaurusCorpusReader\n",
            "28. MTECorpusReader\n",
            "29. MWAPPDBCorpusReader\n",
            "30. MacMorphoCorpusReader\n",
            "31. NKJPCorpusReader\n",
            "32. NPSChatCorpusReader\n",
            "33. NombankCorpusReader\n",
            "34. NonbreakingPrefixesCorpusReader\n",
            "35. OpinionLexiconCorpusReader\n",
            "36. PPAttachmentCorpusReader\n",
            "37. PanLexLiteCorpusReader\n",
            "38. PanlexSwadeshCorpusReader\n",
            "39. Pl196xCorpusReader\n",
            "40. PlaintextCorpusReader\n",
            "41. PortugueseCategorizedPlaintextCorpusReader\n",
            "42. PropbankCorpusReader\n",
            "43. ProsConsCorpusReader\n",
            "44. RTECorpusReader\n",
            "45. ReviewsCorpusReader\n",
            "46. SemcorCorpusReader\n",
            "47. SensevalCorpusReader\n",
            "48. SentiWordNetCorpusReader\n",
            "49. SinicaTreebankCorpusReader\n",
            "50. StringCategoryCorpusReader\n",
            "51. SwadeshCorpusReader\n",
            "52. SwitchboardCorpusReader\n",
            "53. SyntaxCorpusReader\n",
            "54. TaggedCorpusReader\n",
            "55. TimitCorpusReader\n",
            "56. TimitTaggedCorpusReader\n",
            "57. ToolboxCorpusReader\n",
            "58. TwitterCorpusReader\n",
            "59. UdhrCorpusReader\n",
            "60. UnicharsCorpusReader\n",
            "61. VerbnetCorpusReader\n",
            "62. WordListCorpusReader\n",
            "63. WordNetCorpusReader\n",
            "64. WordNetICCorpusReader\n",
            "65. XMLCorpusReader\n",
            "66. YCOECorpusReader\n",
            "67. abc\n",
            "68. alpino\n",
            "69. bcp47\n",
            "70. brown\n",
            "71. cess_cat\n",
            "72. cess_esp\n",
            "73. cmudict\n",
            "74. comparative_sentences\n",
            "75. comtrans\n",
            "76. conll2000\n",
            "77. conll2002\n",
            "78. conll2007\n",
            "79. crubadan\n",
            "80. dependency_treebank\n",
            "81. extended_omw\n",
            "82. floresta\n",
            "83. framenet\n",
            "84. framenet15\n",
            "85. gazetteers\n",
            "86. genesis\n",
            "87. gutenberg\n",
            "88. ieer\n",
            "89. inaugural\n",
            "90. indian\n",
            "91. jeita\n",
            "92. knbc\n",
            "93. lin_thesaurus\n",
            "94. mac_morpho\n",
            "95. machado\n",
            "96. masc_tagged\n",
            "97. movie_reviews\n",
            "98. multext_east\n",
            "99. names\n",
            "100. nombank\n",
            "101. nombank_ptb\n",
            "102. nonbreaking_prefixes\n",
            "103. nps_chat\n",
            "104. opinion_lexicon\n",
            "105. perluniprops\n",
            "106. ppattach\n",
            "107. product_reviews_1\n",
            "108. product_reviews_2\n",
            "109. propbank\n",
            "110. propbank_ptb\n",
            "111. pros_cons\n",
            "112. ptb\n",
            "113. qc\n",
            "114. reuters\n",
            "115. rte\n",
            "116. semcor\n",
            "117. senseval\n",
            "118. sentence_polarity\n",
            "119. sentiwordnet\n",
            "120. shakespeare\n",
            "121. sinica_treebank\n",
            "122. state_union\n",
            "123. stopwords\n",
            "124. subjectivity\n",
            "125. swadesh\n",
            "126. swadesh110\n",
            "127. swadesh207\n",
            "128. switchboard\n",
            "129. timit\n",
            "130. timit_tagged\n",
            "131. toolbox\n",
            "132. treebank\n",
            "133. treebank_chunk\n",
            "134. treebank_raw\n",
            "135. twitter_samples\n",
            "136. udhr\n",
            "137. udhr2\n",
            "138. universal_treebanks\n",
            "139. verbnet\n",
            "140. webtext\n",
            "141. wordnet\n",
            "142. wordnet2021\n",
            "143. wordnet2022\n",
            "144. wordnet31\n",
            "145. wordnet_ic\n",
            "146. words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "pBbqo3KLcPn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_unique_synonyms(word):\n",
        "    \"\"\"Get all unique synonyms (lemmas) for a given word using WordNet.\"\"\"\n",
        "    synonyms = set()  #\n",
        "\n",
        "\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            synonyms.add(synonym.lower())\n",
        "\n",
        "    return sorted(synonyms)\n",
        "\n",
        "\n",
        "word = \"car\"\n",
        "synonyms = get_unique_synonyms(word)\n",
        "\n",
        "\n",
        "print(f\"Unique synonyms for '{word}':\")\n",
        "for i, synonym in enumerate(synonyms, 1):\n",
        "    print(f\"{i}. {synonym}\")\n",
        "\n",
        "print(f\"\\nTotal unique synonyms found: {len(synonyms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMwYTRDcQgw",
        "outputId": "167552cd-8709-46da-ee5e-5ba74f682974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique synonyms for 'car':\n",
            "1. auto\n",
            "2. automobile\n",
            "3. cable car\n",
            "4. car\n",
            "5. elevator car\n",
            "6. gondola\n",
            "7. machine\n",
            "8. motorcar\n",
            "9. railcar\n",
            "10. railroad car\n",
            "11. railway car\n",
            "\n",
            "Total unique synonyms found: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def get_unique_synonyms(word):\n",
        "    \"\"\"Get all unique synonyms (lemmas) for a given word using WordNet.\"\"\"\n",
        "    synonyms = set()\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            synonyms.add(synonym.lower())\n",
        "    return sorted(synonyms)\n",
        "\n",
        "def replace_with_synonyms_random(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged = pos_tag(words)\n",
        "\n",
        "    replaceable_words = []\n",
        "    for i, (word, tag) in enumerate(tagged):\n",
        "        if word.isalpha():\n",
        "            synonyms = get_unique_synonyms(word)\n",
        "            filtered_synonyms = [s for s in synonyms if s != word.lower()]\n",
        "            if filtered_synonyms:\n",
        "                replaceable_words.append((i, filtered_synonyms))\n",
        "\n",
        "    if replaceable_words:\n",
        "        n = random.randint(1, len(replaceable_words))\n",
        "        random.shuffle(replaceable_words)\n",
        "\n",
        "        for i, synonyms in replaceable_words[:n]:\n",
        "            words[i] = random.choice(synonyms)\n",
        "\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nLO-gBITu0S",
        "outputId": "7f90039c-c881-4c61-e8d0-67326b942903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Enter Sentence: - \")\n",
        "new_sentence = replace_with_synonyms_random(sentence)\n",
        "print(\"Original:\", sentence)\n",
        "print(\"Modified:\", new_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFu3oZUUX87_",
        "outputId": "773fcbbc-73bd-4398-c9d4-f25ec29436a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Sentence: - The smart student solved a difficult problem quickly.\n",
            "Original: The smart student solved a difficult problem quickly.\n",
            "Modified: The smart educatee solved a hard job rapidly .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "name_entity_recog = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "def extract_entities(word):\n",
        "  doc = name_entity_recog(word)\n",
        "  print(\"Entities found: \")\n",
        "  for ent in doc.ents:\n",
        "    print(f\"{ent.text} --> {ent.label_}\")"
      ],
      "metadata": {
        "id": "zMbN6Sbfa97T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Narendra Modi was born in Gujarat and is serving as prime minister of the India.\"\n",
        "extract_entities(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ7CMXTCc2lO",
        "outputId": "555ebfbd-d19f-4407-b6c9-9d04c31f6507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found: \n",
            "Narendra Modi --> PERSON\n",
            "Gujarat --> GPE\n",
            "India --> GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "name_entity_recog = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample replacements for some entity types\n",
        "entity_replacements = {\n",
        "    \"PERSON\": [\"Elon Musk\", \"Taylor Swift\", \"Barack Obama\", \"Emma Watson\"],\n",
        "    \"GPE\": [\"India\", \"Germany\", \"Brazil\", \"Japan\", \"Canada\"],\n",
        "    \"LOC\": [\"Sahara Desert\", \"Amazon Rainforest\", \"Mount Everest\"]\n",
        "}\n",
        "\n",
        "def extract_and_replace_entities(text):\n",
        "    doc = name_entity_recog(text)\n",
        "    print(\"Entities found:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"{ent.text} --> {ent.label_}\")\n",
        "\n",
        "    new_text = text\n",
        "    replaced_entities = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        ent_type = ent.label_\n",
        "        ent_text = ent.text\n",
        "\n",
        "        if ent_type in entity_replacements:\n",
        "            if ent_text not in replaced_entities:\n",
        "                options = [e for e in entity_replacements[ent_type] if e.lower() != ent_text.lower()]\n",
        "                if options:\n",
        "                    replaced_entities[ent_text] = random.choice(options)\n",
        "            if ent_text in replaced_entities:\n",
        "                new_text = new_text.replace(ent_text, replaced_entities[ent_text])\n",
        "\n",
        "    print(\"\\nModified Text:\")\n",
        "    print(new_text)\n"
      ],
      "metadata": {
        "id": "Hf3fXiEme4xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Narendra Modi was born in Gujarat and is serving as prime minister of the India.\"\n",
        "extract_and_replace_entities(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AlKch9Je_n9",
        "outputId": "ada60e58-61e2-40db-e83a-2b8d80f6af9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found:\n",
            "Narendra Modi --> PERSON\n",
            "Gujarat --> GPE\n",
            "India --> GPE\n",
            "\n",
            "Modified Text:\n",
            "Emma Watson was born in Canada and is serving as prime minister of the Brazil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WhiteSpace tokenizer"
      ],
      "metadata": {
        "id": "BBeWNHH4GdAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "def tokenizing_word(word):\n",
        "  tokenizer = WhitespaceTokenizer()\n",
        "  return tokenizer.tokenize(word)\n",
        "\n",
        "\n",
        "tokens = tokenizing_word(input(\"Enter senetence: \"))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkpwcqEGpKYN",
        "outputId": "165cdf59-d7ac-464e-e5e3-fc520407b2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter senetence: Yash is living in Berlin, Germany \n",
            "['Yash', 'is', 'living', 'in', 'Berlin,', 'Germany']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE tokenizer"
      ],
      "metadata": {
        "id": "JBJOBXgKGVv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialise a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Trainer\n",
        "trainer = BpeTrainer(\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "    vocab_size=1000  # Target vocabulary size\n",
        ")\n",
        "\n",
        "\n",
        "sample_texts = [\n",
        "    \"This is a simple example of Byte-Pair Encoding.\",\n",
        "    \"Hugging Face provides great NLP tools.\",\n",
        "    \"Tokenization is an important NLP task.\",\n",
        "    \"BPE helps handle rare words effectively.\"\n",
        "]\n",
        "\n",
        "\n",
        "tokenizer.train_from_iterator(sample_texts, trainer)\n",
        "\n",
        "\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "\n",
        "\n",
        "\n",
        "test_sentence = \"This is a new sentence with some unknownwords.\"\n",
        "encoding = tokenizer.encode(test_sentence)\n",
        "\n",
        "print(\"\\nOriginal sentence:\", test_sentence)\n",
        "print(\"Tokenized output:\", encoding.tokens)\n",
        "print(\"Token IDs:\", encoding.ids)"
      ],
      "metadata": {
        "id": "by0kvEG_DT__",
        "outputId": "02f361ab-a4ad-4c91-c848-3eb11625439b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original sentence: This is a new sentence with some unknownwords.\n",
            "Tokenized output: ['This', 'is', 'a', 'n', 'e', 'w', 's', 'en', 't', 'en', 'ce', 'w', 'i', 't', 'h', 's', 'o', 'm', 'e', 'u', 'n', 'k', 'n', 'o', 'w', 'n', 'words', '.']\n",
            "Token IDs: [106, 39, 15, 26, 18, 34, 30, 70, 31, 70, 62, 34, 22, 31, 21, 30, 27, 25, 18, 32, 26, 23, 26, 27, 34, 26, 123, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords"
      ],
      "metadata": {
        "id": "qVU1ZjN8GSDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary tools from NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the required data (only needed once)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove stop words from text\"\"\"\n",
        "\n",
        "    # Step 1: Split the text into individual words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Step 2: Get English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Step 3: Keep only non-stop words (and ignore punctuation)\n",
        "    clean_words = []\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words and word.isalpha():\n",
        "            clean_words.append(word)\n",
        "\n",
        "    # Step 4: Combine the words back into a sentence\n",
        "    return ' '.join(clean_words)\n"
      ],
      "metadata": {
        "id": "YUw3JLmNDdb2",
        "outputId": "7f4ca726-2008-44ad-90be-6761f447187b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"The quick brown fox jumps over the lazy dog\"\n",
        "print(f\"Original: {sample}\")\n",
        "print(f\"Cleaned: {clean_text(sample)}\")"
      ],
      "metadata": {
        "id": "WcLOfj-bFWqu",
        "outputId": "90cb075a-0941-4236-f13b-21c35b968ecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The quick brown fox jumps over the lazy dog\n",
            "Cleaned: quick brown fox jumps lazy dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misspelled English word using NLTK"
      ],
      "metadata": {
        "id": "PLznjgyBG8-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "def spell_checker(misspelled_word, max_distance=2):\n",
        "\n",
        "    # Ensuring we have the required NLTK data\n",
        "    try:\n",
        "        nltk.data.find('corpora/words')\n",
        "    except LookupError:\n",
        "        nltk.download('words')\n",
        "\n",
        "    # Getting English vocabulary\n",
        "    english_vocab = set(words.words())\n",
        "\n",
        "    # If the word is already correct\n",
        "    if misspelled_word.lower() in english_vocab:\n",
        "        return misspelled_word\n",
        "\n",
        "    # Finding similar words within edit distance\n",
        "    suggestions = []\n",
        "    for word in english_vocab:\n",
        "        if abs(len(word) - len(misspelled_word)) <= max_distance:\n",
        "            distance = edit_distance(misspelled_word.lower(), word.lower())\n",
        "            if distance <= max_distance:\n",
        "                suggestions.append((distance, word))\n",
        "\n",
        "    # Sort suggestions by edit distance and frequency and Get the closest matches (smallest edit distance)\n",
        "    if suggestions:\n",
        "        suggestions.sort()\n",
        "        closest_distance = suggestions[0][0]\n",
        "        closest_matches = [word for (dist, word) in suggestions if dist == closest_distance]\n",
        "\n",
        "        # Return the shortest among closest matches\n",
        "        return min(closest_matches, key=len)\n",
        "\n",
        "    return misspelled_word\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_words = [\"accomodate\", \"recieve\", \"adress\", \"beleive\", \"wierd\", \"independant\"]\n",
        "\n",
        "    for word in test_words:\n",
        "        corrected = spell_checker(word)\n",
        "        print(f\"Original: {word:15} → Corrected: {corrected}\")"
      ],
      "metadata": {
        "id": "iSQkEpfvFh63",
        "outputId": "2d3aedf2-4a89-4b71-926a-1cdf9ee51af0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: accomodate      → Corrected: accommodate\n",
            "Original: recieve         → Corrected: relieve\n",
            "Original: adress          → Corrected: dress\n",
            "Original: beleive         → Corrected: belive\n",
            "Original: wierd           → Corrected: wird\n",
            "Original: independant     → Corrected: independent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misspelled English word using TextBlob"
      ],
      "metadata": {
        "id": "xvvqPy1DUYN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def correct_spelling(sentence):\n",
        "    \"\"\"Correct spelling in a given sentence\"\"\"\n",
        "    blob = TextBlob(sentence)\n",
        "    return str(blob.correct())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_sentence = input(\"Enter a sentence to check: \")\n",
        "    corrected = correct_spelling(input_sentence)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nOriginal:\", input_sentence)\n",
        "    print(\"Corrected:\", corrected)"
      ],
      "metadata": {
        "id": "9zS7ZJI3Ub4B",
        "outputId": "b211a0a0-3825-4f09-adfa-ce1a180aa51c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence to check: I beleive that the independant goverment recieve a lot of accomodation\n",
            "\n",
            "Original: I beleive that the independant goverment recieve a lot of accomodation\n",
            "Corrected: I believe that the independent government receive a lot of accommodation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and lemmatization"
      ],
      "metadata": {
        "id": "T17NY4Rqa68U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def compare_processes(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    print(f\"\\n{'Word':<15}{'Stem':<15}{'Lemma':<15}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for word in words:\n",
        "        if word.isalpha():\n",
        "            print(f\"{word.lower():<15}{stemmer.stem(word):<15}{lemmatizer.lemmatize(word):<15}\")\n",
        "\n",
        "sample_text = \"The mice were playing with leaves and boxes under the tallest trees\"\n",
        "print(\"Original Text:\", sample_text)\n",
        "compare_processes(sample_text)"
      ],
      "metadata": {
        "id": "7ODaH-YabCIN",
        "outputId": "90ef3538-13ab-4271-d867-a7568dfce9a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The mice were playing with leaves and boxes under the tallest trees\n",
            "\n",
            "Word           Stem           Lemma          \n",
            "----------------------------------------\n",
            "the            the            The            \n",
            "mice           mice           mouse          \n",
            "were           were           were           \n",
            "playing        play           playing        \n",
            "with           with           with           \n",
            "leaves         leav           leaf           \n",
            "and            and            and            \n",
            "boxes          box            box            \n",
            "under          under          under          \n",
            "the            the            the            \n",
            "tallest        tallest        tallest        \n",
            "trees          tree           tree           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-speech using spaCy and NLTk"
      ],
      "metadata": {
        "id": "1hnt-23gb1fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp  = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tagging_spacy(sentence):\n",
        "    \"\"\"Performs POS tagging using spaCy.\"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    print(\"POS tagging with spaCy:\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<10} {token.pos_:<10} {token.tag_}\")\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "pos_tagging_spacy(sentence)"
      ],
      "metadata": {
        "id": "PedBZMH6d-1L",
        "outputId": "33af76ed-6645-4408-a474-3b38a3697c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging with spaCy:\n",
            "The        DET        DT\n",
            "quick      ADJ        JJ\n",
            "brown      ADJ        JJ\n",
            "fox        NOUN       NN\n",
            "jumps      VERB       VBZ\n",
            "over       ADP        IN\n",
            "the        DET        DT\n",
            "lazy       ADJ        JJ\n",
            "dog        NOUN       NN\n",
            ".          PUNCT      .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging_nltk(sentence):\n",
        "  \"\"\"Performs POS tagging using NLTK.\"\"\"\n",
        "\n",
        "  words = word_tokenize(sentence)\n",
        "  pos_tags = pos_tag(words)\n",
        "\n",
        "  print(\"POS tagging with NLTK:\")\n",
        "  for word, tag in pos_tags:\n",
        "    print(f\"{word:<10} {tag}\")\n",
        "\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "pos_tagging_nltk(f\"\\n{sentence}\")"
      ],
      "metadata": {
        "id": "vORXb3MOd_wV",
        "outputId": "9db2f0f4-d603-4194-8c00-91e9e5a3d9bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging with NLTK:\n",
            "The        DT\n",
            "quick      JJ\n",
            "brown      NN\n",
            "fox        NN\n",
            "jumps      VBZ\n",
            "over       IN\n",
            "the        DT\n",
            "lazy       JJ\n",
            "dog        NN\n",
            ".          .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spam detection form kaggle"
      ],
      "metadata": {
        "id": "GeK_wT2_rXCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "# Download the dataset\n",
        "print(\"Downloading dataset...\")\n",
        "path = kagglehub.dataset_download(\"smayanj/spam-detection-dataset\")\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "\n",
        "csv_filename = 'spam_detection_dataset.csv'\n",
        "\n",
        "# Construct the full path to the CSV file\n",
        "csv_file_path = os.path.join(path, csv_filename)\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "print(f\"\\nLoading dataset from: {csv_file_path}\")\n",
        "df = pd.read_csv(csv_file_path, encoding='latin-1') # Adjust encoding if needed\n",
        "\n",
        "# --- Display the top 5 rows ---\n",
        "print(\"\\nTop 5 rows of the dataset:\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "TXSzvT7_vbrS",
        "outputId": "27ce36de-c046-407d-e080-9c6e91e4b0a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Dataset downloaded to: /kaggle/input/spam-detection-dataset\n",
            "\n",
            "Loading dataset from: /kaggle/input/spam-detection-dataset/spam_detection_dataset.csv\n",
            "\n",
            "Top 5 rows of the dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   num_links  num_words  has_offer  sender_score  all_caps  is_spam\n",
              "0          3         98          1      0.718607         0        0\n",
              "1          0        170          0      0.698901         1        0\n",
              "2          0         38          0      0.620466         0        0\n",
              "3          0        116          0      0.701755         0        0\n",
              "4          3         89          1      0.583621         1        1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7bf9219-467b-4a0f-b9ba-03195c4687bf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_links</th>\n",
              "      <th>num_words</th>\n",
              "      <th>has_offer</th>\n",
              "      <th>sender_score</th>\n",
              "      <th>all_caps</th>\n",
              "      <th>is_spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>98</td>\n",
              "      <td>1</td>\n",
              "      <td>0.718607</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "      <td>0.698901</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>0.620466</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "      <td>0.701755</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>89</td>\n",
              "      <td>1</td>\n",
              "      <td>0.583621</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7bf9219-467b-4a0f-b9ba-03195c4687bf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7bf9219-467b-4a0f-b9ba-03195c4687bf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7bf9219-467b-4a0f-b9ba-03195c4687bf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a4c81b20-3d88-4df9-a76d-7a7b0948fdae\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4c81b20-3d88-4df9-a76d-7a7b0948fdae')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a4c81b20-3d88-4df9-a76d-7a7b0948fdae button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"num_links\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_words\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47,\n        \"min\": 38,\n        \"max\": 170,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          170,\n          89\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_offer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sender_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05911679582719927,\n        \"min\": 0.5836211903463085,\n        \"max\": 0.718607000019532,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6989012256305066,\n          0.5836211903463085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"all_caps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_spam\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}