{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhqFfPMga1Jbu++iiD/zX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simarjit1303/Data-Science/blob/main/exercises/machine-learning/Big-Data-Analytics/Big_Data_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus\n",
        "\n",
        "corpus_names = [name for name in dir(nltk.corpus) if not name.startswith('_') and hasattr(getattr(nltk.corpus, name), 'fileids')]\n",
        "\n",
        "# Print each identified corpus name on a new line\n",
        "for i, name in enumerate(corpus_names):\n",
        "    print(f\"{i+1}. {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi_QJk7YYNo3",
        "outputId": "d9324dce-e4e6-44d7-982e-c1565a04017f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. AlignedCorpusReader\n",
            "2. AlpinoCorpusReader\n",
            "3. BCP47CorpusReader\n",
            "4. BNCCorpusReader\n",
            "5. BracketParseCorpusReader\n",
            "6. CHILDESCorpusReader\n",
            "7. CMUDictCorpusReader\n",
            "8. CategorizedBracketParseCorpusReader\n",
            "9. CategorizedCorpusReader\n",
            "10. CategorizedPlaintextCorpusReader\n",
            "11. CategorizedSentencesCorpusReader\n",
            "12. CategorizedTaggedCorpusReader\n",
            "13. ChasenCorpusReader\n",
            "14. ChunkedCorpusReader\n",
            "15. ComparativeSentencesCorpusReader\n",
            "16. ConllChunkCorpusReader\n",
            "17. ConllCorpusReader\n",
            "18. CorpusReader\n",
            "19. CrubadanCorpusReader\n",
            "20. DependencyCorpusReader\n",
            "21. EuroparlCorpusReader\n",
            "22. FramenetCorpusReader\n",
            "23. IEERCorpusReader\n",
            "24. IPIPANCorpusReader\n",
            "25. IndianCorpusReader\n",
            "26. KNBCorpusReader\n",
            "27. LinThesaurusCorpusReader\n",
            "28. MTECorpusReader\n",
            "29. MWAPPDBCorpusReader\n",
            "30. MacMorphoCorpusReader\n",
            "31. NKJPCorpusReader\n",
            "32. NPSChatCorpusReader\n",
            "33. NombankCorpusReader\n",
            "34. NonbreakingPrefixesCorpusReader\n",
            "35. OpinionLexiconCorpusReader\n",
            "36. PPAttachmentCorpusReader\n",
            "37. PanLexLiteCorpusReader\n",
            "38. PanlexSwadeshCorpusReader\n",
            "39. Pl196xCorpusReader\n",
            "40. PlaintextCorpusReader\n",
            "41. PortugueseCategorizedPlaintextCorpusReader\n",
            "42. PropbankCorpusReader\n",
            "43. ProsConsCorpusReader\n",
            "44. RTECorpusReader\n",
            "45. ReviewsCorpusReader\n",
            "46. SemcorCorpusReader\n",
            "47. SensevalCorpusReader\n",
            "48. SentiWordNetCorpusReader\n",
            "49. SinicaTreebankCorpusReader\n",
            "50. StringCategoryCorpusReader\n",
            "51. SwadeshCorpusReader\n",
            "52. SwitchboardCorpusReader\n",
            "53. SyntaxCorpusReader\n",
            "54. TaggedCorpusReader\n",
            "55. TimitCorpusReader\n",
            "56. TimitTaggedCorpusReader\n",
            "57. ToolboxCorpusReader\n",
            "58. TwitterCorpusReader\n",
            "59. UdhrCorpusReader\n",
            "60. UnicharsCorpusReader\n",
            "61. VerbnetCorpusReader\n",
            "62. WordListCorpusReader\n",
            "63. WordNetCorpusReader\n",
            "64. WordNetICCorpusReader\n",
            "65. XMLCorpusReader\n",
            "66. YCOECorpusReader\n",
            "67. abc\n",
            "68. alpino\n",
            "69. bcp47\n",
            "70. brown\n",
            "71. cess_cat\n",
            "72. cess_esp\n",
            "73. cmudict\n",
            "74. comparative_sentences\n",
            "75. comtrans\n",
            "76. conll2000\n",
            "77. conll2002\n",
            "78. conll2007\n",
            "79. crubadan\n",
            "80. dependency_treebank\n",
            "81. extended_omw\n",
            "82. floresta\n",
            "83. framenet\n",
            "84. framenet15\n",
            "85. gazetteers\n",
            "86. genesis\n",
            "87. gutenberg\n",
            "88. ieer\n",
            "89. inaugural\n",
            "90. indian\n",
            "91. jeita\n",
            "92. knbc\n",
            "93. lin_thesaurus\n",
            "94. mac_morpho\n",
            "95. machado\n",
            "96. masc_tagged\n",
            "97. movie_reviews\n",
            "98. multext_east\n",
            "99. names\n",
            "100. nombank\n",
            "101. nombank_ptb\n",
            "102. nonbreaking_prefixes\n",
            "103. nps_chat\n",
            "104. opinion_lexicon\n",
            "105. perluniprops\n",
            "106. ppattach\n",
            "107. product_reviews_1\n",
            "108. product_reviews_2\n",
            "109. propbank\n",
            "110. propbank_ptb\n",
            "111. pros_cons\n",
            "112. ptb\n",
            "113. qc\n",
            "114. reuters\n",
            "115. rte\n",
            "116. semcor\n",
            "117. senseval\n",
            "118. sentence_polarity\n",
            "119. sentiwordnet\n",
            "120. shakespeare\n",
            "121. sinica_treebank\n",
            "122. state_union\n",
            "123. stopwords\n",
            "124. subjectivity\n",
            "125. swadesh\n",
            "126. swadesh110\n",
            "127. swadesh207\n",
            "128. switchboard\n",
            "129. timit\n",
            "130. timit_tagged\n",
            "131. toolbox\n",
            "132. treebank\n",
            "133. treebank_chunk\n",
            "134. treebank_raw\n",
            "135. twitter_samples\n",
            "136. udhr\n",
            "137. udhr2\n",
            "138. universal_treebanks\n",
            "139. verbnet\n",
            "140. webtext\n",
            "141. wordnet\n",
            "142. wordnet2021\n",
            "143. wordnet2022\n",
            "144. wordnet31\n",
            "145. wordnet_ic\n",
            "146. words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "pBbqo3KLcPn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_unique_synonyms(word):\n",
        "    \"\"\"Get all unique synonyms (lemmas) for a given word using WordNet.\"\"\"\n",
        "    synonyms = set()  #\n",
        "\n",
        "\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            synonyms.add(synonym.lower())\n",
        "\n",
        "    return sorted(synonyms)\n",
        "\n",
        "\n",
        "word = \"car\"\n",
        "synonyms = get_unique_synonyms(word)\n",
        "\n",
        "\n",
        "print(f\"Unique synonyms for '{word}':\")\n",
        "for i, synonym in enumerate(synonyms, 1):\n",
        "    print(f\"{i}. {synonym}\")\n",
        "\n",
        "print(f\"\\nTotal unique synonyms found: {len(synonyms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMwYTRDcQgw",
        "outputId": "167552cd-8709-46da-ee5e-5ba74f682974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique synonyms for 'car':\n",
            "1. auto\n",
            "2. automobile\n",
            "3. cable car\n",
            "4. car\n",
            "5. elevator car\n",
            "6. gondola\n",
            "7. machine\n",
            "8. motorcar\n",
            "9. railcar\n",
            "10. railroad car\n",
            "11. railway car\n",
            "\n",
            "Total unique synonyms found: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def get_unique_synonyms(word):\n",
        "    \"\"\"Get all unique synonyms (lemmas) for a given word using WordNet.\"\"\"\n",
        "    synonyms = set()\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            synonyms.add(synonym.lower())\n",
        "    return sorted(synonyms)\n",
        "\n",
        "def replace_with_synonyms_random(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged = pos_tag(words)\n",
        "\n",
        "    replaceable_words = []\n",
        "    for i, (word, tag) in enumerate(tagged):\n",
        "        if word.isalpha():\n",
        "            synonyms = get_unique_synonyms(word)\n",
        "            filtered_synonyms = [s for s in synonyms if s != word.lower()]\n",
        "            if filtered_synonyms:\n",
        "                replaceable_words.append((i, filtered_synonyms))\n",
        "\n",
        "    if replaceable_words:\n",
        "        n = random.randint(1, len(replaceable_words))\n",
        "        random.shuffle(replaceable_words)\n",
        "\n",
        "        for i, synonyms in replaceable_words[:n]:\n",
        "            words[i] = random.choice(synonyms)\n",
        "\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nLO-gBITu0S",
        "outputId": "7f90039c-c881-4c61-e8d0-67326b942903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Enter Sentence: - \")\n",
        "new_sentence = replace_with_synonyms_random(sentence)\n",
        "print(\"Original:\", sentence)\n",
        "print(\"Modified:\", new_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFu3oZUUX87_",
        "outputId": "773fcbbc-73bd-4398-c9d4-f25ec29436a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Sentence: - The smart student solved a difficult problem quickly.\n",
            "Original: The smart student solved a difficult problem quickly.\n",
            "Modified: The smart educatee solved a hard job rapidly .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "name_entity_recog = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "def extract_entities(word):\n",
        "  doc = name_entity_recog(word)\n",
        "  print(\"Entities found: \")\n",
        "  for ent in doc.ents:\n",
        "    print(f\"{ent.text} --> {ent.label_}\")"
      ],
      "metadata": {
        "id": "zMbN6Sbfa97T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Narendra Modi was born in Gujarat and is serving as prime minister of the India.\"\n",
        "extract_entities(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ7CMXTCc2lO",
        "outputId": "555ebfbd-d19f-4407-b6c9-9d04c31f6507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found: \n",
            "Narendra Modi --> PERSON\n",
            "Gujarat --> GPE\n",
            "India --> GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "name_entity_recog = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample replacements for some entity types\n",
        "entity_replacements = {\n",
        "    \"PERSON\": [\"Elon Musk\", \"Taylor Swift\", \"Barack Obama\", \"Emma Watson\"],\n",
        "    \"GPE\": [\"India\", \"Germany\", \"Brazil\", \"Japan\", \"Canada\"],\n",
        "    \"LOC\": [\"Sahara Desert\", \"Amazon Rainforest\", \"Mount Everest\"]\n",
        "}\n",
        "\n",
        "def extract_and_replace_entities(text):\n",
        "    doc = name_entity_recog(text)\n",
        "    print(\"Entities found:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"{ent.text} --> {ent.label_}\")\n",
        "\n",
        "    new_text = text\n",
        "    replaced_entities = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        ent_type = ent.label_\n",
        "        ent_text = ent.text\n",
        "\n",
        "        if ent_type in entity_replacements:\n",
        "            if ent_text not in replaced_entities:\n",
        "                options = [e for e in entity_replacements[ent_type] if e.lower() != ent_text.lower()]\n",
        "                if options:\n",
        "                    replaced_entities[ent_text] = random.choice(options)\n",
        "            if ent_text in replaced_entities:\n",
        "                new_text = new_text.replace(ent_text, replaced_entities[ent_text])\n",
        "\n",
        "    print(\"\\nModified Text:\")\n",
        "    print(new_text)\n"
      ],
      "metadata": {
        "id": "Hf3fXiEme4xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Narendra Modi was born in Gujarat and is serving as prime minister of the India.\"\n",
        "extract_and_replace_entities(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AlKch9Je_n9",
        "outputId": "ada60e58-61e2-40db-e83a-2b8d80f6af9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found:\n",
            "Narendra Modi --> PERSON\n",
            "Gujarat --> GPE\n",
            "India --> GPE\n",
            "\n",
            "Modified Text:\n",
            "Emma Watson was born in Canada and is serving as prime minister of the Brazil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WhiteSpace tokenizer"
      ],
      "metadata": {
        "id": "BBeWNHH4GdAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "def tokenizing_word(word):\n",
        "  tokenizer = WhitespaceTokenizer()\n",
        "  return tokenizer.tokenize(word)\n",
        "\n",
        "\n",
        "tokens = tokenizing_word(input(\"Enter senetence: \"))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkpwcqEGpKYN",
        "outputId": "165cdf59-d7ac-464e-e5e3-fc520407b2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter senetence: Yash is living in Berlin, Germany \n",
            "['Yash', 'is', 'living', 'in', 'Berlin,', 'Germany']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE tokenizer"
      ],
      "metadata": {
        "id": "JBJOBXgKGVv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialise a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Trainer\n",
        "trainer = BpeTrainer(\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "    vocab_size=1000  # Target vocabulary size\n",
        ")\n",
        "\n",
        "\n",
        "sample_texts = [\n",
        "    \"This is a simple example of Byte-Pair Encoding.\",\n",
        "    \"Hugging Face provides great NLP tools.\",\n",
        "    \"Tokenization is an important NLP task.\",\n",
        "    \"BPE helps handle rare words effectively.\"\n",
        "]\n",
        "\n",
        "\n",
        "tokenizer.train_from_iterator(sample_texts, trainer)\n",
        "\n",
        "\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "\n",
        "\n",
        "\n",
        "test_sentence = \"This is a new sentence with some unknownwords.\"\n",
        "encoding = tokenizer.encode(test_sentence)\n",
        "\n",
        "print(\"\\nOriginal sentence:\", test_sentence)\n",
        "print(\"Tokenized output:\", encoding.tokens)\n",
        "print(\"Token IDs:\", encoding.ids)"
      ],
      "metadata": {
        "id": "by0kvEG_DT__",
        "outputId": "02f361ab-a4ad-4c91-c848-3eb11625439b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original sentence: This is a new sentence with some unknownwords.\n",
            "Tokenized output: ['This', 'is', 'a', 'n', 'e', 'w', 's', 'en', 't', 'en', 'ce', 'w', 'i', 't', 'h', 's', 'o', 'm', 'e', 'u', 'n', 'k', 'n', 'o', 'w', 'n', 'words', '.']\n",
            "Token IDs: [106, 39, 15, 26, 18, 34, 30, 70, 31, 70, 62, 34, 22, 31, 21, 30, 27, 25, 18, 32, 26, 23, 26, 27, 34, 26, 123, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords"
      ],
      "metadata": {
        "id": "qVU1ZjN8GSDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary tools from NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the required data (only needed once)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove stop words from text\"\"\"\n",
        "\n",
        "    # Step 1: Split the text into individual words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Step 2: Get English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Step 3: Keep only non-stop words (and ignore punctuation)\n",
        "    clean_words = []\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words and word.isalpha():\n",
        "            clean_words.append(word)\n",
        "\n",
        "    # Step 4: Combine the words back into a sentence\n",
        "    return ' '.join(clean_words)\n"
      ],
      "metadata": {
        "id": "YUw3JLmNDdb2",
        "outputId": "7f4ca726-2008-44ad-90be-6761f447187b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"The quick brown fox jumps over the lazy dog\"\n",
        "print(f\"Original: {sample}\")\n",
        "print(f\"Cleaned: {clean_text(sample)}\")"
      ],
      "metadata": {
        "id": "WcLOfj-bFWqu",
        "outputId": "90cb075a-0941-4236-f13b-21c35b968ecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The quick brown fox jumps over the lazy dog\n",
            "Cleaned: quick brown fox jumps lazy dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misspelled English word using NLTK"
      ],
      "metadata": {
        "id": "PLznjgyBG8-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "def spell_checker(misspelled_word, max_distance=2):\n",
        "\n",
        "    # Ensuring we have the required NLTK data\n",
        "    try:\n",
        "        nltk.data.find('corpora/words')\n",
        "    except LookupError:\n",
        "        nltk.download('words')\n",
        "\n",
        "    # Getting English vocabulary\n",
        "    english_vocab = set(words.words())\n",
        "\n",
        "    # If the word is already correct\n",
        "    if misspelled_word.lower() in english_vocab:\n",
        "        return misspelled_word\n",
        "\n",
        "    # Finding similar words within edit distance\n",
        "    suggestions = []\n",
        "    for word in english_vocab:\n",
        "        if abs(len(word) - len(misspelled_word)) <= max_distance:\n",
        "            distance = edit_distance(misspelled_word.lower(), word.lower())\n",
        "            if distance <= max_distance:\n",
        "                suggestions.append((distance, word))\n",
        "\n",
        "    # Sort suggestions by edit distance and frequency and Get the closest matches (smallest edit distance)\n",
        "    if suggestions:\n",
        "        suggestions.sort()\n",
        "        closest_distance = suggestions[0][0]\n",
        "        closest_matches = [word for (dist, word) in suggestions if dist == closest_distance]\n",
        "\n",
        "        # Return the shortest among closest matches\n",
        "        return min(closest_matches, key=len)\n",
        "\n",
        "    return misspelled_word\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_words = [\"accomodate\", \"recieve\", \"adress\", \"beleive\", \"wierd\", \"independant\"]\n",
        "\n",
        "    for word in test_words:\n",
        "        corrected = spell_checker(word)\n",
        "        print(f\"Original: {word:15} → Corrected: {corrected}\")"
      ],
      "metadata": {
        "id": "iSQkEpfvFh63",
        "outputId": "2d3aedf2-4a89-4b71-926a-1cdf9ee51af0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: accomodate      → Corrected: accommodate\n",
            "Original: recieve         → Corrected: relieve\n",
            "Original: adress          → Corrected: dress\n",
            "Original: beleive         → Corrected: belive\n",
            "Original: wierd           → Corrected: wird\n",
            "Original: independant     → Corrected: independent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misspelled English word using TextBlob"
      ],
      "metadata": {
        "id": "xvvqPy1DUYN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def correct_spelling(sentence):\n",
        "    \"\"\"Correct spelling in a given sentence\"\"\"\n",
        "    blob = TextBlob(sentence)\n",
        "    return str(blob.correct())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_sentence = input(\"Enter a sentence to check: \")\n",
        "    corrected = correct_spelling(input_sentence)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nOriginal:\", input_sentence)\n",
        "    print(\"Corrected:\", corrected)"
      ],
      "metadata": {
        "id": "9zS7ZJI3Ub4B",
        "outputId": "b211a0a0-3825-4f09-adfa-ce1a180aa51c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence to check: I beleive that the independant goverment recieve a lot of accomodation\n",
            "\n",
            "Original: I beleive that the independant goverment recieve a lot of accomodation\n",
            "Corrected: I believe that the independent government receive a lot of accommodation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and lemmatization"
      ],
      "metadata": {
        "id": "T17NY4Rqa68U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def compare_processes(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    print(f\"\\n{'Word':<15}{'Stem':<15}{'Lemma':<15}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for word in words:\n",
        "        if word.isalpha():\n",
        "            print(f\"{word.lower():<15}{stemmer.stem(word):<15}{lemmatizer.lemmatize(word):<15}\")\n",
        "\n",
        "sample_text = \"The mice were playing with leaves and boxes under the tallest trees\"\n",
        "print(\"Original Text:\", sample_text)\n",
        "compare_processes(sample_text)"
      ],
      "metadata": {
        "id": "7ODaH-YabCIN",
        "outputId": "90ef3538-13ab-4271-d867-a7568dfce9a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The mice were playing with leaves and boxes under the tallest trees\n",
            "\n",
            "Word           Stem           Lemma          \n",
            "----------------------------------------\n",
            "the            the            The            \n",
            "mice           mice           mouse          \n",
            "were           were           were           \n",
            "playing        play           playing        \n",
            "with           with           with           \n",
            "leaves         leav           leaf           \n",
            "and            and            and            \n",
            "boxes          box            box            \n",
            "under          under          under          \n",
            "the            the            the            \n",
            "tallest        tallest        tallest        \n",
            "trees          tree           tree           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-speech using spaCy and NLTk"
      ],
      "metadata": {
        "id": "1hnt-23gb1fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp  = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tagging_spacy(sentence):\n",
        "    \"\"\"Performs POS tagging using spaCy.\"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    print(\"POS tagging with spaCy:\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<10} {token.pos_:<10} {token.tag_}\")\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "pos_tagging_spacy(sentence)"
      ],
      "metadata": {
        "id": "PedBZMH6d-1L",
        "outputId": "33af76ed-6645-4408-a474-3b38a3697c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging with spaCy:\n",
            "The        DET        DT\n",
            "quick      ADJ        JJ\n",
            "brown      ADJ        JJ\n",
            "fox        NOUN       NN\n",
            "jumps      VERB       VBZ\n",
            "over       ADP        IN\n",
            "the        DET        DT\n",
            "lazy       ADJ        JJ\n",
            "dog        NOUN       NN\n",
            ".          PUNCT      .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging_nltk(sentence):\n",
        "  \"\"\"Performs POS tagging using NLTK.\"\"\"\n",
        "\n",
        "  words = word_tokenize(sentence)\n",
        "  pos_tags = pos_tag(words)\n",
        "\n",
        "  print(\"POS tagging with NLTK:\")\n",
        "  for word, tag in pos_tags:\n",
        "    print(f\"{word:<10} {tag}\")\n",
        "\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "pos_tagging_nltk(f\"\\n{sentence}\")"
      ],
      "metadata": {
        "id": "vORXb3MOd_wV",
        "outputId": "9db2f0f4-d603-4194-8c00-91e9e5a3d9bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging with NLTK:\n",
            "The        DT\n",
            "quick      JJ\n",
            "brown      NN\n",
            "fox        NN\n",
            "jumps      VBZ\n",
            "over       IN\n",
            "the        DT\n",
            "lazy       JJ\n",
            "dog        NN\n",
            ".          .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11/06/2025 Word2Vec with Gensim"
      ],
      "metadata": {
        "id": "ns5RWCmtdkVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim scipy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xkLjQsQgf4QD",
        "outputId": "1e438299-0f50-4144-ef6f-4527e5acf156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.0\n",
            "    Uninstalling numpy-2.3.0:\n",
            "      Successfully uninstalled numpy-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "# Sample sentences for training (replace with your actual text data)\n",
        "sentences = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"i love natural language processing\",\n",
        "    \"word embeddings are powerful\",\n",
        "    \"gensim provides useful NLP tools\"\n",
        "]\n",
        "\n",
        "# Preprocess the sentences (tokenize and lowercase)\n",
        "processed_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "# Initialize and train the Word2Vec model\n",
        "# vector_size: Dimension of word vectors\n",
        "# window: Context window size\n",
        "# min_count: Ignores all words with total frequency lower than this\n",
        "# workers: Use these many worker threads to train the model\n",
        "model = Word2Vec(sentences=processed_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Train the model\n",
        "model.train(processed_sentences, total_examples=len(processed_sentences), epochs=10)\n",
        "\n",
        "# Now you can use the trained model, for example to find similar words\n",
        "print(\"Words similar to 'fox':\")\n",
        "print(model.wv.most_similar(\"fox\"))\n",
        "\n",
        "print(\"\\nVector for 'dog':\")\n",
        "print(model.wv['dog'])"
      ],
      "metadata": {
        "id": "G7yBtlFMdq6O",
        "outputId": "3bb12ca7-0f13-4a06-c8b9-6c0ea9ced849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'fox':\n",
            "[('word', 0.25290533900260925), ('quick', 0.17022386193275452), ('processing', 0.15014882385730743), ('jumps', 0.1390596479177475), ('powerful', 0.10849529504776001), ('over', 0.034675031900405884), ('provides', 0.03308531641960144), ('are', 0.016055237501859665), ('language', 0.013865072280168533), ('natural', 0.00446228077635169)]\n",
            "\n",
            "Vector for 'dog':\n",
            "[-9.5756073e-03  8.9417165e-03  4.1676052e-03  9.2376601e-03\n",
            "  6.6426201e-03  2.9251529e-03  9.8066498e-03 -4.4259652e-03\n",
            " -6.8056989e-03  4.2255968e-03  3.7299057e-03 -5.6647756e-03\n",
            "  9.7064096e-03 -3.5577805e-03  9.5491437e-03  8.3532231e-04\n",
            " -6.3360897e-03 -1.9762206e-03 -7.3789754e-03 -2.9835640e-03\n",
            "  1.0419341e-03  9.4846943e-03  9.3581704e-03 -6.5985210e-03\n",
            "  3.4733845e-03  2.2778565e-03 -2.4908043e-03 -9.2287110e-03\n",
            "  1.0253822e-03 -8.1641264e-03  6.3197366e-03 -5.7981531e-03\n",
            "  5.5353511e-03  9.8328451e-03 -1.6182919e-04  4.5286771e-03\n",
            " -1.8120423e-03  7.3595271e-03  3.9373497e-03 -9.0114363e-03\n",
            " -2.3950513e-03  3.6274483e-03 -9.8599594e-05 -1.2014590e-03\n",
            " -1.0526023e-03 -1.6718677e-03  6.0192484e-04  4.1630249e-03\n",
            " -4.2511616e-03 -3.8339968e-03 -5.5244585e-05  2.6764721e-04\n",
            " -1.6750515e-04 -4.7874227e-03  4.3109353e-03 -2.1708407e-03\n",
            "  2.1008386e-03  6.6631474e-04  5.9691281e-03 -6.8421876e-03\n",
            " -6.8138456e-03 -4.4779717e-03  9.4362451e-03 -1.5892964e-03\n",
            " -9.4296876e-03 -5.4243766e-04 -4.4526644e-03  6.0023437e-03\n",
            " -9.5856637e-03  2.8568443e-03 -9.2527894e-03  1.2507183e-03\n",
            "  6.0004042e-03  7.3978649e-03 -7.6219817e-03 -6.0538505e-03\n",
            " -6.8358355e-03 -7.9192268e-03 -9.4993124e-03 -2.1268141e-03\n",
            " -8.3691947e-04 -7.2576953e-03  6.7839846e-03  1.1208284e-03\n",
            "  5.8293818e-03  1.4713996e-03  7.9220982e-04 -7.3666261e-03\n",
            " -2.1754389e-03  4.3207817e-03 -5.0854338e-03  1.1298184e-03\n",
            "  2.8807272e-03 -1.5347835e-03  9.9321986e-03  8.3500333e-03\n",
            "  2.4165427e-03  7.1165287e-03  5.8926274e-03 -5.5812597e-03]\n"
          ]
        }
      ]
    }
  ]
}