{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simarjit1303/Data-Science/blob/main/exercises/machine-learning/supervised-learning/titanic_competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "korean-wednesday",
      "metadata": {
        "id": "korean-wednesday"
      },
      "source": [
        "# Titanic Competition\n",
        "You should build an end-to-end machine learning pipeline to predict survivors of the Titanic disaster and participate in the corresponding Kaggle competition. In particular, you should do the following:\n",
        "- Read the Titanic competition page on [Kaggle](https://www.kaggle.com/competitions/titanic/overview).\n",
        "- Load the `titanic` dataset using [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). You can find this dataset in the datasets folder.\n",
        "- Split the dataset into training and test sets using [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
        "- Build an end-to-end machine learning pipeline, including all necessary steps, to have a running solution with some performance.\n",
        "- Collaborate with your groupmates to finalize your pipeline by\n",
        "    - reading the discussion forum to learn from other community members;\n",
        "    - discussing the bottlenecks of your current solution;\n",
        "    - running experiments on your pipeline;\n",
        "    - improving the performance of your pipeline.\n",
        "- Test the best pipeline on the test set and report various [evaluation metrics](https://scikit-learn.org/0.15/modules/model_evaluation.html).  \n",
        "- Present your pipeline.\n",
        "- Submit your predictions to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "infrared-copper",
      "metadata": {
        "id": "infrared-copper"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "# classifiers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploartion and Preprocessing"
      ],
      "metadata": {
        "id": "RcUcYeGe6HQD"
      },
      "id": "RcUcYeGe6HQD"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/m-mahdavi/teaching/refs/heads/main/datasets/titanic.csv')\n",
        "# dataset.head(10)"
      ],
      "metadata": {
        "id": "M-dvGiGk1U0N"
      },
      "id": "M-dvGiGk1U0N",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.describe()"
      ],
      "metadata": {
        "id": "UIYQaKRix63h"
      },
      "id": "UIYQaKRix63h",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking missing values\n",
        "# dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "w-g2gDl_02Hf"
      },
      "id": "w-g2gDl_02Hf",
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Addind missing values in 'Age' with the median\n",
        "# dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n",
        "\n",
        "# # Adding missing values in 'Embarked' with the mode\n",
        "# dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# # Impute missing values in 'Cabin' with 'Unknown'\n",
        "# dataset['Cabin'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# # Verifing if there are any remaining missing values\n",
        "# dataset.isnull().sum()\n"
      ],
      "metadata": {
        "id": "U1nQDeGr4Kae"
      },
      "id": "U1nQDeGr4Kae",
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "38MoTLXm6x_c"
      },
      "id": "38MoTLXm6x_c"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Title Extraction\n",
        "dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "# 2. Creating a new column to put family under one category\n",
        "dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
        "\n",
        "# 3. IsAlone\n",
        "dataset['IsAlone'] = 0\n",
        "dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "\n",
        "# 4. Grouping passengers into categories\n",
        "dataset['AgeBin'] = pd.cut(dataset['Age'], bins=[0, 12, 20, 60, 100], labels=['Child', 'Young Adult', 'Adult', 'Senior'])\n",
        "dataset['FareBin'] = pd.qcut(dataset['Fare'], 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "dataset['CabinDeck'] = dataset['Cabin'].str[0].fillna('U')\n"
      ],
      "metadata": {
        "id": "kCXXxSWN60aJ"
      },
      "id": "kCXXxSWN60aJ",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "F8DFpZed9ume"
      },
      "id": "F8DFpZed9ume"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Defining features and target\n",
        "numerical_features = ['Pclass', 'Age', 'Fare', 'FamilySize', 'IsAlone']\n",
        "categorical_features = ['Sex', 'Embarked', 'Title', 'AgeBin', 'FareBin', 'CabinDeck']\n",
        "target = 'Survived'\n",
        "\n",
        "# 2. Spliiting data into test and train data\n",
        "X = dataset[numerical_features + categorical_features]\n",
        "y = dataset[target]\n",
        "X_train_data, X_test_data, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# 4. Create preprocessing pipelines\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', KNNImputer(n_neighbors=5)),\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "iik6SyWM9g1s"
      },
      "id": "iik6SyWM9g1s",
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Modelling"
      ],
      "metadata": {
        "id": "JFXbaLvlASKH"
      },
      "id": "JFXbaLvlASKH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models and hyperparameter grids\n",
        "models = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(random_state=42),\n",
        "        'param_dist': {\n",
        "            'classifier__C': [0.1, 1, 10]\n",
        "        }\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'model': DecisionTreeClassifier(random_state=42),\n",
        "        'param_dist': {\n",
        "            'classifier__max_depth': [3, 5, 7],\n",
        "            'classifier__min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'param_dist': {\n",
        "            'classifier__n_estimators': [50,100],\n",
        "            'classifier__max_depth': [3, 5, 7],\n",
        "            'classifier__min_samples_split': [5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4],\n",
        "            'classifier__max_features': ['auto','sqrt', 'log2']\n",
        "        }\n",
        "    },\n",
        "    'Support Vector Machine': {\n",
        "        'model': SVC(random_state=42),\n",
        "        'param_dist': {\n",
        "            'classifier__C': [0.1, 1, 10],\n",
        "            'classifier__kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'param_dist': {\n",
        "            'classifier__n_neighbors': [3, 5, 7],\n",
        "            'classifier__weights': ['uniform', 'distance']\n",
        "        }\n",
        "    },\n",
        "    'Naive Bayes': {\n",
        "        'model': GaussianNB(),\n",
        "        'param_dist': {}\n",
        "    }\n",
        "    # 'XGBoost': {\n",
        "    #     'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "    #     'param_dist': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}\n",
        "    # }\n",
        "}\n",
        "\n",
        "#  Perform Randomized Search CV for each model\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for model_name, model_data in models.items():\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model_data['model'])\n",
        "    ])\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_distributions=model_data['param_dist'],\n",
        "        n_iter=50,\n",
        "        scoring='accuracy',\n",
        "        cv=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train_data, y_train)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = random_search.predict(X_test_data)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{model_name} Accuracy (Test): {accuracy}\")\n",
        "    print(f\"{model_name} Best Hyperparameters: {random_search.best_params_}\\n\")\n",
        "\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = random_search.best_estimator_\n",
        "        best_model_name = model_name\n",
        "\n",
        "# Print the best model and its accuracy\n",
        "print(f\"The best model is: {best_model_name} with accuracy: {best_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "OrWfb-0hAKEI",
        "outputId": "989886e3-3c20-42ad-89f7-fe5c33971751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OrWfb-0hAKEI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=50. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy (Test): 0.8116591928251121\n",
            "Logistic Regression Best Hyperparameters: {'classifier__C': 1}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 9 is smaller than n_iter=50. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy (Test): 0.820627802690583\n",
            "Decision Tree Best Hyperparameters: {'classifier__min_samples_split': 2, 'classifier__max_depth': 3}\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}